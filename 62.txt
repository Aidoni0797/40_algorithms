Вопрос 3. Как алгоритм будет работать с большими наборами данных?

Для получения результата алгоритм обрабатывает данные определенным образом. Как правило, по мере увеличения объема
данных требуется все больше времени для их обработки и вычисления результатов. Существуют наборы данных,
представляющие особую сложность для инфраструктур и алгоритмов из-за их объема, разнообразия и скорости.
Иногда их обозначают термином "большие данные" (big data). Хорошо разработанный алгоритм должен быть 
масштабируемым: по возможности эффективно рабоать, использовать доступные ресурсы и генерировать правильные
результаты в разумные сроки. Архитектура алгоритма становится еще более важной при работе с большими данными.
Чтобы количественно оценить масштабируемость алгоритма, необходимо учитывать следующие два аспекта:

- Увеличение потребностей в ресурсах по мере рота объема входных данных. Оценка такого требования называется анализом
пространственной сложности.

- Увеличение времени, затрачиваемого на выполнение, по мере роста объема входных данных. Оценка данного 
параметра называется анализом временной сложности.

Мы живем в эпоху лавинообразного роста объемов данных. Термин "большие данные" стал общепринятым, поскольку
он отражает размер и сложность данных, которые обычно требуется обрабатывать современным алгоритмам.

На этапе разработки и тестирования многие алгоритмы используют лишь небольшую выборку данных, поэтому важно
учитывать аспект масштабируемости. В частности, необходимо тщателно проанализировать (протестировать или
спрогнозировать) то, как на производительность алгоритма повлияет увеличение объема данных.
